{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish reading data\n",
      "time elapsed: 0.018239974975585938\n",
      "11\n",
      "9734\n",
      "start processing: 1\n",
      "start processing: 0\n",
      "start processing: 1003\n",
      "start processing: 1004\n",
      "bad sample: 1004\n",
      "article: ['applications -- the nonlinear nature of ferroelectric materials can be used to make capacitors with tunable capacitance .', 'catalytic properties of ferroelectrics have been studied since 1952 when parravano observed anomalies in co oxidation rates over ferroelectric sodium and potassium niobates near the curie temperature of these materials .', 'sabatier principle states that the surface - adsorbates interaction has to be an optimal amount : not too weak to be inert toward the reactants and not too strong to poison the surface and avoid desorption of the products : a compromise situation .', \"this set of optimum interactions is usually referred to as `` top of the volcano '' in activity volcano plots .\", 'on the other hand , ferroelectric polarization - dependent chemistry can offer the possibility of switching the surface -- adsorbates interaction from strong adsorption to strong desorption , thus a compromise between desorption and adsorption is no longer needed .', 'ferroelectric polarization can also act as an energy harvester .', 'polarization can help the separation of photo - generated electron - hole pairs , leading to enhanced photocatalysis', 'materials -- the internal electric dipoles of a ferroelectric material are coupled to the material lattice so anything that changes the lattice will change the strength of the dipoles -lrb- in other words , a change in the spontaneous polarization -rrb- .', 'the change in the spontaneous polarization results in a change in the surface charge .', \"based on ginzburg -- landau theory , the free energy of a ferroelectric material , in the absence of an electric field and applied stress may be written as a taylor expansion in terms of the order parameter , `` p '' .\", 'if a sixth order expansion is used -lrb- i.e']\n",
      "tuples: [['capacitors', 'is with', 'tunable capacitance', '15', '16', '16', '17', '17', '19', '1.000', '-- polarization applications -- the nonlinear nature of ferroelectric materials can be used to make capacitors with tunable capacitance .', ': NN NNS : DT JJ NN IN JJ NNS MD VB VBN TO VB NNS IN JJ NN .', 'capacitor', 'be with', 'tunable capacitance']]\n",
      "start processing: 10\n",
      "start processing: 100\n",
      "start processing: 1000\n",
      "start processing: 1001\n",
      "start processing: 1007\n",
      "start processing: 101\n",
      "start processing: 1008\n",
      "average node num: 187.72727272727272\n",
      "average edge num: 101.18181818181819\n",
      "average node num after pruned: 17.90909090909091\n",
      "average edge num after pruned: 16.272727272727273\n",
      "average sum worthy nodes: 51.36363636363637\n",
      "average in salient sent nodes: 0.0\n",
      "average sum worthy nodes after pruned: 6.909090909090909\n",
      "average in salient sent nodes after pruned: 0.0\n",
      "average sum worthy edges: 39.90909090909091\n",
      "average in salient sent edges: 0.0\n",
      "average sum worthy edges after pruned: 8.636363636363637\n",
      "average in salient sent edges after pruned: 0.0\n",
      "===== /home/xueweiwa/Capstone/xueweiwa/data/shorten/val/41.json\n",
      "===== /home/xueweiwa/Capstone/xueweiwa/data/shorten/val/284.json\n",
      "===== /home/xueweiwa/Capstone/xueweiwa/data/shorten/val/342.json\n",
      "===== /home/xueweiwa/Capstone/xueweiwa/data/shorten/val/344.json\n",
      "===== /home/xueweiwa/Capstone/xueweiwa/data/shorten/val/410.json\n",
      "===== /home/xueweiwa/Capstone/xueweiwa/data/shorten/val/411.json\n",
      "===== /home/xueweiwa/Capstone/xueweiwa/data/shorten/val/412.json\n",
      "===== /home/xueweiwa/Capstone/xueweiwa/data/shorten/val/413.json\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "import multiprocessing as mp\n",
    "import time, glob\n",
    "from cytoolz import curry\n",
    "from datetime import timedelta\n",
    "import gc\n",
    "import os\n",
    "from nltk.stem import porter\n",
    "\n",
    "\n",
    "\n",
    "def read_raw(split):\n",
    "    data_dict = {}\n",
    "    with open(os.path.join(OPENIE_DIR,   split + '_res.tsv'), 'r') as f:\n",
    "        for line in f:\n",
    "            data = line.strip().split('\\t')\n",
    "            data = [_.strip() for _ in data]\n",
    "            try:\n",
    "                data_dict[data[0]][data[1]].append(data[2:])\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    data_dict[data[0]].setdefault(data[1], [data[2:]])\n",
    "                except:\n",
    "                    data_dict[data[0]] = {data[1]: [data[2:]]}\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_raw_2(split):\n",
    "    data_dict = {}\n",
    "    with open(os.path.join(OPENIE_DIR, split + '_out.txt'), 'r') as f:\n",
    "        for line in f:\n",
    "            data = line.strip().split('\\t')\n",
    "            data = [_.strip() for _ in data]\n",
    "            data_dict.setdefault(data[0], [])\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_data(filename, split='train'):\n",
    "    with open(filename, 'r') as g:\n",
    "        data = json.load(g)\n",
    "    coref = data['input_mention_cluster']\n",
    "    abstract = data['abstract']\n",
    "    article = data['article_new']\n",
    "#     if split != 'test':\n",
    "#         extracts = data['extracted_combine']\n",
    "#     else:\n",
    "    extracts = []\n",
    "    abstract = ' '.join(abstract)\n",
    "    id2node = {}\n",
    "    _id = 0\n",
    "    for entity in coref:\n",
    "        entities = []\n",
    "#         for mention in entity:\n",
    "#             print(mention)\n",
    "        entities.append((entity['text'], entity['position']))\n",
    "        _id += 1\n",
    "        id2node.setdefault('entity_' + str(_id), entities)\n",
    "\n",
    "    return abstract, id2node, data, article, extracts\n",
    "\n",
    "def filter_arguments(f_arguments, t_arguments, s_arguments):\n",
    "    if len(f_arguments) > 1:\n",
    "        f_arguments = list(f_arguments)\n",
    "        f_arguments.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "        start = int(f_arguments[0][2])\n",
    "        end = int(f_arguments[0][3])\n",
    "        new_f_arguments = [f_arguments[0]]\n",
    "        for candidate in f_arguments:\n",
    "            if ((int(candidate[2]) > start - 1) and (int(candidate[3]) < end + 1)) or (start > int(candidate[2]) - 1 and end < int(candidate[3]) + 1):\n",
    "                continue\n",
    "            else:\n",
    "                new_f_arguments.append(candidate)\n",
    "        f_arguments = new_f_arguments\n",
    "    if len(t_arguments) > 1:\n",
    "        t_arguments = list(t_arguments)\n",
    "        t_arguments.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "        start = int(t_arguments[0][1])\n",
    "        end = int(t_arguments[0][2])\n",
    "        new_t_arguments = [t_arguments[0]]\n",
    "        for candidate in t_arguments:\n",
    "            if (int(candidate[1]) > start - 2 and int(candidate[2]) < end + 2) or (start > int(candidate[1]) - 1 and end < int(candidate[2]) + 1):\n",
    "                continue\n",
    "            else:\n",
    "                new_t_arguments.append(candidate)\n",
    "        t_arguments = new_t_arguments\n",
    "    if len(s_arguments) > 1:\n",
    "        s_arguments = list(s_arguments)\n",
    "        s_arguments.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "        start = int(s_arguments[0][2])\n",
    "        end = int(s_arguments[0][3])\n",
    "        new_s_arguments = [s_arguments[0]]\n",
    "        for candidate in s_arguments:\n",
    "            if (int(candidate[2]) > start - 2 and int(candidate[3]) < end + 2) or (start > int(candidate[2]) - 1 and end < int(candidate[3]) + 1):\n",
    "                continue\n",
    "            else:\n",
    "                new_s_arguments.append(candidate)\n",
    "        s_arguments = new_s_arguments\n",
    "    return f_arguments, t_arguments, s_arguments\n",
    "\n",
    "\n",
    "def merge(clusters):\n",
    "    new_cluster = []\n",
    "    length = len(clusters)\n",
    "    not_finished = list(range(length))\n",
    "    while(len(not_finished) > 0):\n",
    "        target = not_finished[0]\n",
    "        not_finished.remove(target)\n",
    "        target_cluster = clusters[target]\n",
    "        new_not_finished = []\n",
    "        for _id in not_finished:\n",
    "            cluster = clusters[_id]\n",
    "            if len(set(cluster) & set(target_cluster)) > 0:\n",
    "                target_cluster += cluster\n",
    "            else:\n",
    "                new_not_finished.append(_id)\n",
    "        not_finished = new_not_finished\n",
    "        new_cluster.append(set(target_cluster))\n",
    "    merged = new_cluster\n",
    "    return merged\n",
    "\n",
    "def find_share_mention(tuples): # rule 2\n",
    "    sentd_ids = set(list(range(len(tuples))))\n",
    "    # find aligned tuples\n",
    "    clusters = []\n",
    "    for id, tuple in enumerate(tuples):\n",
    "        for _id, _tuple in enumerate(tuples):\n",
    "            if _id < id + 1:\n",
    "                continue\n",
    "            if (_tuple[0] == tuple[0] and _tuple[1] == tuple[1]) or (\n",
    "                    _tuple[0] == tuple[0] and _tuple[2] == tuple[2]) or (\n",
    "                    _tuple[2] == tuple[2] and _tuple[1] == tuple[1]):\n",
    "                clusters.append((id, _id))\n",
    "    final_clusters = merge(clusters)\n",
    "\n",
    "    for cluster in final_clusters:\n",
    "        sentd_ids = sentd_ids - cluster\n",
    "\n",
    "    final_clusters = [list(final_cluster) for final_cluster in final_clusters]\n",
    "    no_merge_cluster = [[sentd_id] for sentd_id in sentd_ids]\n",
    "    final_sent_clusters = no_merge_cluster + final_clusters\n",
    "    return final_sent_clusters\n",
    "\n",
    "\n",
    "def filter_argument(tuples, thresold=10): # rule0\n",
    "    new_tuples = []\n",
    "    for tuple in tuples:\n",
    "        if int(tuple[4]) - int(tuple[3]) < thresold:\n",
    "            if int(tuple[6]) - int(tuple[5]) < thresold:\n",
    "                if int(tuple[8]) - int(tuple[7]) < thresold:\n",
    "                    new_tuples.append(tuple)\n",
    "    return new_tuples\n",
    "\n",
    "def filter_tuples(final_sent_clusters, tuples, entities, sent_id): # rule 3\n",
    "    arguments_list = []\n",
    "    for cluster in final_sent_clusters:\n",
    "        f_arguments = set()\n",
    "        t_arguments = set()\n",
    "        s_arguments = set()\n",
    "        for i in range(len(cluster)):\n",
    "            flag_1 = 0\n",
    "            canonical_mention_1 = ' '\n",
    "            for _id, mention_cluster in entities.items():\n",
    "                for mention, _sent_id in mention_cluster:\n",
    "                    _sent_id = _sent_id[0]\n",
    "                    if int(_sent_id) == int(sent_id):\n",
    "                        if tuples[cluster[i]][0] in mention:\n",
    "                            canonical_mention_1 = _id\n",
    "                            flag_1 = 1\n",
    "                            break\n",
    "\n",
    "            f_arguments.add((tuples[cluster[i]][0], canonical_mention_1, int(tuples[cluster[i]][3]), int(tuples[cluster[i]][4])))\n",
    "\n",
    "            t_arguments.add((tuples[cluster[i]][1], int(tuples[cluster[i]][5]), int(tuples[cluster[i]][6])))\n",
    "\n",
    "            canonical_mention_2 = ' '\n",
    "            flag_2 = 0\n",
    "            for _id, mention_cluster in entities.items():\n",
    "                for mention, _sent_id in mention_cluster:\n",
    "                    _sent_id = _sent_id[0]\n",
    "                    if int(_sent_id) == int(sent_id):\n",
    "                        if tuples[cluster[i]][2] in mention:\n",
    "                            canonical_mention_2 = _id\n",
    "                            flag_2 = 1\n",
    "                            break\n",
    "            if (flag_1 and flag_2) and canonical_mention_1 == canonical_mention_2:\n",
    "                s_arguments.add((tuples[cluster[i]][2], ' ', int(tuples[cluster[i]][7]), int(tuples[cluster[i]][8])))\n",
    "            else:\n",
    "                s_arguments.add(\n",
    "                    (tuples[cluster[i]][2], canonical_mention_2, int(tuples[cluster[i]][7]), int(tuples[cluster[i]][8])))\n",
    "\n",
    "        # align arguments\n",
    "        unfinished = []\n",
    "        if len(f_arguments) + len(t_arguments) + len(s_arguments) > 3:\n",
    "            unfinished.append((list(f_arguments), list(t_arguments), list(s_arguments)))\n",
    "        else:\n",
    "            arguments_list.append((list(f_arguments)[0], list(t_arguments)[0], list(s_arguments)[0]))\n",
    "        while len(unfinished) > 0:\n",
    "            f_arguments, t_arguments, s_arguments = unfinished.pop()\n",
    "            f_arguments, t_arguments, s_arguments = filter_arguments(f_arguments, t_arguments, s_arguments)\n",
    "            if len(f_arguments) > 1:\n",
    "                unfinished.append((f_arguments[1:], t_arguments, s_arguments))\n",
    "                f_arguments, t_arguments, s_arguments = [f_arguments[0]], t_arguments, s_arguments\n",
    "            if len(t_arguments) > 1:\n",
    "                unfinished.append((f_arguments, t_arguments[1:], s_arguments))\n",
    "                f_arguments, t_arguments, s_arguments = f_arguments, [t_arguments[0]], s_arguments\n",
    "            if len(s_arguments) > 1:\n",
    "                unfinished.append((f_arguments, t_arguments, s_arguments[1:]))\n",
    "                f_arguments, t_arguments, s_arguments = f_arguments, t_arguments, [s_arguments[0]]\n",
    "            arguments_list.append((f_arguments[0], t_arguments[0], s_arguments[0]))\n",
    "    return arguments_list\n",
    "\n",
    "def filter_stopwords(arguments, stopwords):\n",
    "    def filter_one(words):\n",
    "        return all([word.strip() in stopwords for word in words])\n",
    "    new_arguments = []\n",
    "    for argument in arguments:\n",
    "        flags = [filter_one(argument[0][1].lower().split(' ') + argument[0][0].lower().split(' ')),\n",
    "                 filter_one(argument[1][0].lower().split(' ')), filter_one(argument[2][1].lower().split(' ')  + argument[2][0].lower().split(' '))]\n",
    "        if not all(flags):\n",
    "            new_arguments.append(argument)\n",
    "    return new_arguments\n",
    "\n",
    "def get_stopwods():\n",
    "    stopwords = []\n",
    "#     with open('/home/luyang/stopwords.txt', 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             stopwords.append(line.strip())\n",
    "#     stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    return stopwords\n",
    "\n",
    "def final_process(all_arguments, entities, article):\n",
    "    finished_entity = set()\n",
    "    entity2node = {}\n",
    "    nodes = {}\n",
    "    edges = {}\n",
    "    word_nums = [len(sent.split(' ')) for sent in article]\n",
    "    _id = 0\n",
    "    _id_edge = 0\n",
    "    for sent_id, arguments in all_arguments:\n",
    "        sent = article[sent_id].split()\n",
    "        start_num = sum(word_nums[:sent_id])\n",
    "\n",
    "        for argument in arguments:\n",
    "            e1 = argument[0]\n",
    "            r = argument[1]\n",
    "            e2 = argument[2]\n",
    "            # process e1\n",
    "            if e1[1] != ' ':\n",
    "                if e1[1] not in finished_entity:\n",
    "                    entity = entities[e1[1]]\n",
    "                    entity_processed = []\n",
    "                    for mention in entity:\n",
    "                        info = {}\n",
    "                        info['text'] = mention[0]\n",
    "                        info['word_pos'] = list(range(int(mention[1][1]), int(mention[1][2])))\n",
    "                        info['insent_pos'] = list(range(int(mention[1][3]), int(mention[1][4])))\n",
    "                        info['sent_pos'] = int(mention[1][0])\n",
    "                        entity_processed.append(info)\n",
    "\n",
    "                    e1_node = 'node_' + str(_id)\n",
    "                    entity2node.setdefault(e1[1], e1_node)\n",
    "                    nodes.setdefault('node_' + str(_id), {'content': entity_processed,\n",
    "                                                          'type': 'entity'})\n",
    "                    _id += 1\n",
    "                    finished_entity.add(e1[1])\n",
    "                else:\n",
    "                    e1_node = entity2node[e1[1]]\n",
    "\n",
    "                # save original form\n",
    "                info = {}\n",
    "                info['text'] = e1[0]\n",
    "                _start = int(e1[2])\n",
    "                _end = int(e1[3])\n",
    "                info['insent_pos'] = []\n",
    "                for word in e1[0].split():\n",
    "                    poses = [i for i, _word in enumerate(sent) if _word == word]\n",
    "                    if len(poses) > 1:\n",
    "                        poses = [i for i in poses if i > _start-1 and i < _end]\n",
    "                    if len(poses) < 1:\n",
    "                        poses.append(-1)\n",
    "                    info['insent_pos'].append(poses[0])\n",
    "                info['word_pos'] = [pos + start_num for pos in info['insent_pos']]\n",
    "                info['sent_pos'] = sent_id\n",
    "                original_e1 = [info]\n",
    "            else:\n",
    "                info = {}\n",
    "                info['text'] = e1[0]\n",
    "                _start = int(e1[2])\n",
    "                _end = int(e1[3])\n",
    "                info['insent_pos'] = []\n",
    "                for word in e1[0].split():\n",
    "                    poses = [i for i, _word in enumerate(sent) if _word == word]\n",
    "                    if len(poses) > 1:\n",
    "                        poses = [i for i in poses if i > _start-1 and i < _end]\n",
    "                    if len(poses) < 1:\n",
    "                        poses.append(-1)\n",
    "                    info['insent_pos'].append(poses[0])\n",
    "                info['word_pos'] = [pos+start_num if pos > -1 else -1 for pos in info['insent_pos']]\n",
    "                info['sent_pos'] = sent_id\n",
    "                e1_node = 'node_' + str(_id)\n",
    "                nodes.setdefault('node_' + str(_id), {'content': [info],\n",
    "                                                      'type': 'other'})\n",
    "                _id += 1\n",
    "                original_e1 = [info]\n",
    "            # process e2\n",
    "            if e2[1] != ' ':\n",
    "                if e2[1] not in finished_entity:\n",
    "                    entity = entities[e2[1]]\n",
    "                    entity_processed = []\n",
    "                    for mention in entity:\n",
    "                        info = {}\n",
    "                        info['text'] = mention[0]\n",
    "                        info['word_pos'] = list(range(int(mention[1][1]), int(mention[1][2])))\n",
    "                        info['insent_pos'] = list(range(int(mention[1][3]), int(mention[1][4])))\n",
    "                        info['sent_pos'] = int(mention[1][0])\n",
    "                        entity_processed.append(info)\n",
    "\n",
    "                    e2_node = 'node_' + str(_id)\n",
    "                    nodes.setdefault('node_' + str(_id), {'content': entity_processed,\n",
    "                                                          'type': 'entity'})\n",
    "                    entity2node.setdefault(e2[1], e2_node)\n",
    "                    _id += 1\n",
    "                    finished_entity.add(e2[1])\n",
    "                else:\n",
    "                    e2_node = entity2node[e2[1]]\n",
    "                # save original form\n",
    "                info = {}\n",
    "                info['text'] = e2[0]\n",
    "                _start = int(e2[2])\n",
    "                _end = int(e2[3])\n",
    "                info['insent_pos'] = []\n",
    "                for word in e2[0].split():\n",
    "                    poses = [i for i, _word in enumerate(sent) if _word == word]\n",
    "                    if len(poses) > 1:\n",
    "                        poses = [i for i in poses if i > _start-1 and i < _end]\n",
    "                    if len(poses) < 1:\n",
    "                        poses.append(-1)\n",
    "                    info['insent_pos'].append(poses[0])\n",
    "                info['word_pos'] = [pos + start_num if pos > -1 else -1 for pos in info['insent_pos']]\n",
    "                info['sent_pos'] = sent_id\n",
    "                original_e2 = [info]\n",
    "            else:\n",
    "                info = {}\n",
    "                info['text'] = e2[0]\n",
    "                _start = int(e2[2])\n",
    "                _end = int(e2[3])\n",
    "                info['insent_pos'] = []\n",
    "                for word in e2[0].split():\n",
    "                    poses = [i for i, _word in enumerate(sent) if _word == word]\n",
    "                    if len(poses) > 1:\n",
    "                        poses = [i for i in poses if i > _start-1 and i < _end]\n",
    "                    if len(poses) < 1:\n",
    "                        poses.append(-1)\n",
    "                    info['insent_pos'].append(poses[0])\n",
    "                info['word_pos'] = [pos+start_num for pos in info['insent_pos']]\n",
    "                info['sent_pos'] = sent_id\n",
    "                e2_node = 'node_' + str(_id)\n",
    "                nodes.setdefault('node_' + str(_id), {'content': [info],\n",
    "                                                      'type': 'other'})\n",
    "                _id += 1\n",
    "                original_e2 = [info]\n",
    "            # process r\n",
    "            info = {}\n",
    "            info['text'] = r[0]\n",
    "            _start = int(r[1])\n",
    "            _end = int(r[2])\n",
    "            info['insent_pos'] = []\n",
    "            for word in r[0].split():\n",
    "                poses = [i for i, _word in enumerate(sent) if _word == word and i > _start-1 and i < _end]\n",
    "                if len(poses) < 1:\n",
    "                    poses.append(-1)\n",
    "                info['insent_pos'].append(poses[0])\n",
    "            info['word_pos'] = [pos+start_num if pos > -1 else -1 for pos in info['insent_pos']]\n",
    "            info['sent_pos'] = sent_id\n",
    "            info['arg1'] = e1_node\n",
    "            info['arg2'] = e2_node\n",
    "            info['arg1_original'] = original_e1\n",
    "            info['arg2_original'] = original_e2\n",
    "            edges.setdefault('edge_' + str(_id_edge), {'content':info,\n",
    "                                                       'type': None})\n",
    "            _id_edge += 1\n",
    "\n",
    "    return nodes, edges\n",
    "\n",
    "\n",
    "def pruning(all_arguments, thresold=3): # prune small subgraphs\n",
    "    argument2triple = {}\n",
    "    triples = []\n",
    "    finished_entity = set()\n",
    "    entity2node = {}\n",
    "    nodes = {}\n",
    "    edges = {}\n",
    "    _id = 0\n",
    "    _id_edge = 0\n",
    "    for sent_id, arguments in all_arguments:\n",
    "        for argument in arguments:\n",
    "            triple = []\n",
    "            e1 = argument[0]\n",
    "            r = argument[1]\n",
    "            e2 = argument[2]\n",
    "            # process e1\n",
    "            if e1[1] != ' ':\n",
    "                if e1[1] not in finished_entity:\n",
    "                    e1_node = 'node_' + str(_id)\n",
    "                    triple.append(_id)\n",
    "                    entity2node[e1[1]] = e1_node\n",
    "                    _id += 1\n",
    "                    finished_entity.add(e1[1])\n",
    "                else:\n",
    "                    e1_node = entity2node[e1[1]]\n",
    "                    triple.append(int(e1_node.split('_')[-1]))\n",
    "            else:\n",
    "                e1_node = 'node_' + str(_id)\n",
    "                triple.append(_id)\n",
    "                _id += 1\n",
    "            # process e2\n",
    "            if e2[1] != ' ':\n",
    "                if e2[1] not in finished_entity:\n",
    "                    e2_node = 'node_' + str(_id)\n",
    "                    entity2node.setdefault(e2[1], e2_node)\n",
    "                    triple.append(_id)\n",
    "                    _id += 1\n",
    "                    finished_entity.add(e2[1])\n",
    "                else:\n",
    "                    e2_node = entity2node[e2[1]]\n",
    "                    triple.append(int(e2_node.split('_')[-1]))\n",
    "            else:\n",
    "                e2_node = 'node_' + str(_id)\n",
    "                triple.append(_id)\n",
    "                _id += 1\n",
    "\n",
    "            triple.append('edge_' + str(_id_edge))\n",
    "            _id_edge += 1\n",
    "            triples.append(triple)\n",
    "            argument2triple.setdefault(argument, triple)\n",
    "    numofnode = _id\n",
    "    graph = [[0 for _ in range(numofnode)] for _ in range(numofnode)]\n",
    "    for triple in triples:\n",
    "        graph[triple[0]][triple[1]] = 1\n",
    "        graph[triple[1]][triple[0]] = 1\n",
    "\n",
    "    def subgraph_count(graph):\n",
    "        size = len(graph)\n",
    "        _map = set()\n",
    "        unfinished = [i for i in range(len(graph))]\n",
    "        queue = [0]  # start from 0\n",
    "        count = 0\n",
    "        groups = []\n",
    "        single_node = 0\n",
    "        while len(unfinished) > 0:\n",
    "            if len(queue) == 0:\n",
    "                groups.append(_map)\n",
    "                if len(_map) == 1:\n",
    "                    single_node += 1\n",
    "                _map = set()\n",
    "                queue.append(unfinished[0])\n",
    "                count += 1\n",
    "            _next = queue.pop()\n",
    "            if len(_map) == 0:\n",
    "                _map.add(_next)\n",
    "                unfinished.remove(_next)\n",
    "            for i in range(size):\n",
    "                if i in unfinished and graph[_next][i] > 0:\n",
    "                    _map.add(i)\n",
    "                    unfinished.remove(i)\n",
    "                    queue.append(i)\n",
    "\n",
    "        if len(_map) > 0:\n",
    "            groups.append(_map)\n",
    "            count += 1\n",
    "            groups = sorted(groups, key=lambda x: len(x), reverse=True)\n",
    "        if len(groups) == 0:\n",
    "            return count, None, single_node, groups\n",
    "        else:\n",
    "            return count, groups[0], single_node, groups\n",
    "\n",
    "    count, lgroup, single, groups = subgraph_count(graph)\n",
    "    banned_nodes = []\n",
    "    for group in groups:\n",
    "        if len(group) < thresold:\n",
    "            for _iii in group:\n",
    "                banned_nodes.append(_iii)\n",
    "\n",
    "    new_all_arguments = []\n",
    "    for sent_id, arguments in all_arguments:\n",
    "        args = []\n",
    "        for argument in arguments:\n",
    "            if argument2triple[argument][0] not in banned_nodes:\n",
    "                args.append(argument)\n",
    "        new_all_arguments.append([sent_id, args])\n",
    "    if len(new_all_arguments) == 0:\n",
    "        new_all_arguments = all_arguments\n",
    "\n",
    "    return new_all_arguments\n",
    "\n",
    "def select_largest_group(all_arguments): # prune small subgraphs\n",
    "    argument2triple = {}\n",
    "    triples = []\n",
    "    finished_entity = set()\n",
    "    entity2node = {}\n",
    "    nodes = {}\n",
    "    edges = {}\n",
    "    _id = 0\n",
    "    _id_edge = 0\n",
    "    for sent_id, arguments in all_arguments:\n",
    "        for argument in arguments:\n",
    "            triple = []\n",
    "            e1 = argument[0]\n",
    "            r = argument[1]\n",
    "            e2 = argument[2]\n",
    "            # process e1\n",
    "            if e1[1] != ' ':\n",
    "                if e1[1] not in finished_entity:\n",
    "                    e1_node = 'node_' + str(_id)\n",
    "                    triple.append(_id)\n",
    "                    entity2node[e1[1]] = e1_node\n",
    "                    _id += 1\n",
    "                    finished_entity.add(e1[1])\n",
    "                else:\n",
    "                    e1_node = entity2node[e1[1]]\n",
    "                    triple.append(int(e1_node.split('_')[-1]))\n",
    "            else:\n",
    "                e1_node = 'node_' + str(_id)\n",
    "                triple.append(_id)\n",
    "                _id += 1\n",
    "            # process e2\n",
    "            if e2[1] != ' ':\n",
    "                if e2[1] not in finished_entity:\n",
    "                    e2_node = 'node_' + str(_id)\n",
    "                    entity2node.setdefault(e2[1], e2_node)\n",
    "                    triple.append(_id)\n",
    "                    _id += 1\n",
    "                    finished_entity.add(e2[1])\n",
    "                else:\n",
    "                    e2_node = entity2node[e2[1]]\n",
    "                    triple.append(int(e2_node.split('_')[-1]))\n",
    "            else:\n",
    "                e2_node = 'node_' + str(_id)\n",
    "                triple.append(_id)\n",
    "                _id += 1\n",
    "\n",
    "            triple.append('edge_' + str(_id_edge))\n",
    "            _id_edge += 1\n",
    "            triples.append(triple)\n",
    "            argument2triple.setdefault(argument, triple)\n",
    "    numofnode = _id\n",
    "    graph = [[0 for _ in range(numofnode)] for _ in range(numofnode)]\n",
    "    for triple in triples:\n",
    "        graph[triple[0]][triple[1]] = 1\n",
    "        graph[triple[1]][triple[0]] = 1\n",
    "\n",
    "    def subgraph_count(graph):\n",
    "        size = len(graph)\n",
    "        _map = set()\n",
    "        unfinished = [i for i in range(len(graph))]\n",
    "        queue = [0]  # start from 0\n",
    "        count = 0\n",
    "        groups = []\n",
    "        single_node = 0\n",
    "        while len(unfinished) > 0:\n",
    "            if len(queue) == 0:\n",
    "                groups.append(_map)\n",
    "                if len(_map) == 1:\n",
    "                    single_node += 1\n",
    "                _map = set()\n",
    "                queue.append(unfinished[0])\n",
    "                count += 1\n",
    "            _next = queue.pop()\n",
    "            if len(_map) == 0:\n",
    "                _map.add(_next)\n",
    "                unfinished.remove(_next)\n",
    "            for i in range(size):\n",
    "                if i in unfinished and graph[_next][i] > 0:\n",
    "                    _map.add(i)\n",
    "                    unfinished.remove(i)\n",
    "                    queue.append(i)\n",
    "\n",
    "        if len(_map) > 0:\n",
    "            groups.append(_map)\n",
    "            count += 1\n",
    "            groups = sorted(groups, key=lambda x: len(x), reverse=True)\n",
    "        if len(groups) == 0:\n",
    "            return count, None, single_node, groups\n",
    "        else:\n",
    "            return count, groups[0], single_node, groups\n",
    "\n",
    "    count, lgroup, single, groups = subgraph_count(graph)\n",
    "    banned_nodes = []\n",
    "    for group in groups[1:]:\n",
    "        for _iii in group:\n",
    "            banned_nodes.append(_iii)\n",
    "\n",
    "    new_all_arguments = []\n",
    "    count = 0\n",
    "    for sent_id, arguments in all_arguments:\n",
    "        args = []\n",
    "        for argument in arguments:\n",
    "            if argument2triple[argument][0] not in banned_nodes:\n",
    "                args.append(argument)\n",
    "                count += 1\n",
    "        if len(args) != 0:\n",
    "            new_all_arguments.append([sent_id, args])\n",
    "    if len(new_all_arguments) == 0:\n",
    "        new_all_arguments = all_arguments\n",
    "\n",
    "    return new_all_arguments\n",
    "\n",
    "\n",
    "def make_summary_worth_node(nodes, abstract, stopwords, extracts, stemmer):\n",
    "    sum_worthy = 0\n",
    "    insalientsent = 0\n",
    "    abstract = abstract.lower().split(' ')\n",
    "    abstract = [stemmer.stem(word.lower()) for word in abstract]\n",
    "    new_nodes = {}\n",
    "    for _id, node in nodes.items():\n",
    "        words = []\n",
    "        node['summary_worthy'] = 0\n",
    "        node['InSalientSent'] = 0\n",
    "        for info in node['content']:\n",
    "            words.extend(info['text'].lower().split(' '))\n",
    "            sent = info['sent_pos']\n",
    "            if sent in extracts:\n",
    "                node['InSalientSent'] = 1\n",
    "\n",
    "        for word in words:\n",
    "            if word not in stopwords and word in abstract:\n",
    "                node['summary_worthy'] = 1\n",
    "                break\n",
    "        if node['InSalientSent']:\n",
    "            insalientsent += 1\n",
    "        if node['summary_worthy']:\n",
    "            sum_worthy += 1\n",
    "        new_nodes.setdefault(_id, node)\n",
    "\n",
    "    return new_nodes, insalientsent, sum_worthy\n",
    "\n",
    "def make_summary_worth_edge(edges, abstract, stopwords, extracts, stemmer):\n",
    "    sum_worthy = 0\n",
    "    insalientsent = 0\n",
    "    abstract = abstract.lower().split(' ')\n",
    "    abstract = [stemmer.stem(word.lower()) for word in abstract]\n",
    "    new_edges = {}\n",
    "    for _id, edge in edges.items():\n",
    "        words = []\n",
    "        edge['summary_worthy'] = 0\n",
    "        edge['InSalientSent'] = 0\n",
    "        info = edge['content']\n",
    "        words.extend(info['text'].lower().split(' '))\n",
    "        sent = info['sent_pos']\n",
    "        if sent in extracts:\n",
    "            edge['InSalientSent'] = 1\n",
    "\n",
    "        for word in words:\n",
    "            if word not in stopwords and word in abstract:\n",
    "                edge['summary_worthy'] = 1\n",
    "                break\n",
    "        if edge['InSalientSent']:\n",
    "            insalientsent += 1\n",
    "        if edge['summary_worthy']:\n",
    "            sum_worthy += 1\n",
    "        new_edges.setdefault(_id, edge)\n",
    "\n",
    "    return new_edges, insalientsent, sum_worthy\n",
    "\n",
    "def get_summary_worth_triple(all_arguments, stopwords, abstract):\n",
    "    new_all_arguments = []\n",
    "    for sent_id, arguments in all_arguments:\n",
    "        new_arguments = []\n",
    "        for argument in arguments:\n",
    "            s = [word.lower() for word in (argument[0][0]).split(' ') if word.lower() not in stopwords]\n",
    "            o = [word.lower() for word in (argument[1][0]).split(' ') if word.lower() not in stopwords]\n",
    "            p = [word.lower() for word in (argument[2][0]).split(' ') if word.lower() not in stopwords]\n",
    "            op_words = o + p\n",
    "            for word in op_words:\n",
    "                if word in abstract:\n",
    "                    new_arguments.append(argument)\n",
    "                    break\n",
    "        if len(new_arguments) != 0:\n",
    "            new_all_arguments.append([sent_id, new_arguments])\n",
    "    if len(new_all_arguments) == 0:\n",
    "        new_all_arguments = select_largest_group(all_arguments)\n",
    "\n",
    "\n",
    "    return new_all_arguments\n",
    "\n",
    "\n",
    "\n",
    "@curry\n",
    "def process_one(split, stopwords, stemmer, data, ground_truth=False, test=False):\n",
    "    key = data[0]\n",
    "    # if int(data[0].split('/')[-1].split('.')[0]) % 1000 == 1:\n",
    "    #     gc.collect()\n",
    "    value = data[1]\n",
    "\n",
    "    _id = int(key.split('/')[-1].split('.')[0])\n",
    "    print('start processing:', _id)\n",
    "    # key = OPENIE_DIR + 'temp/' + str(_id) + '.txt'\n",
    "    # if not data.__contains__(key):\n",
    "    #     filename = os.path.join(DATA_DIR, split, str(_id) + '.json')\n",
    "    #     abstract, entities, all_data, article = prepare_data(filename)\n",
    "    #     all_data['nodes'] = {}\n",
    "    #     all_data['edges'] = {}\n",
    "    #     return 0 # return something\n",
    "\n",
    "    filename = os.path.join(DATA_DIR, split, str(_id) + '.json')\n",
    "    abstract, entities, all_data, article, extracts = prepare_data(filename, split)\n",
    "    all_arguments = []\n",
    "    calibrate = 0\n",
    "    for sent_id, tuples in value.items():\n",
    "        sent_id = int(sent_id) - calibrate\n",
    "        if sent_id > len(article)-1:\n",
    "            sent_id = len(article)\n",
    "            flag = 1\n",
    "            while (flag and sent_id > 1):\n",
    "                if ' '.join(tuples[0][10].strip('\\.| ').split(' ')[:5]) in article[sent_id - 1]:\n",
    "                    calibrate += 1\n",
    "                    flag = 0\n",
    "                sent_id = sent_id - 1\n",
    "            if sent_id < 1:\n",
    "                print('bad sample:', _id)\n",
    "                break\n",
    "        if not ' '.join(tuples[0][10].strip('\\.| ').split(' ')[:5]) in article[sent_id]:\n",
    "            flag = 1\n",
    "            while(flag and sent_id > 1):\n",
    "                if ' '.join(tuples[0][10].strip('\\.| ').split(' ')[:5]) in article[sent_id - 1]:\n",
    "                    calibrate += 1\n",
    "                    flag = 0\n",
    "                sent_id = sent_id - 1\n",
    "            if sent_id < 1:\n",
    "                print('bad sample:', _id)\n",
    "                print('article:', article)\n",
    "                print('tuples:', tuples)\n",
    "                break\n",
    "\n",
    "        new_tuples = filter_argument(tuples) # apply rule 0\n",
    "        final_clusters = find_share_mention(new_tuples)\n",
    "        arguments = filter_tuples(final_clusters, tuples, entities, sent_id)\n",
    "        arguments = filter_stopwords(arguments, stopwords)\n",
    "        all_arguments.append((sent_id, arguments))\n",
    "\n",
    "    if ground_truth:\n",
    "        summary = ' '.join(abstract.lower())\n",
    "        if test:\n",
    "            before = all_arguments\n",
    "        all_arguments = get_summary_worth_triple(all_arguments, stopwords, summary)\n",
    "\n",
    "    pruned_all_arguments = pruning(all_arguments)\n",
    "    nodes_pruned, edges_pruned = final_process(pruned_all_arguments, entities, article)\n",
    "\n",
    "    nodes, edges = final_process(all_arguments, entities, article)\n",
    "    nodes, insalientsent, sum_worthy = make_summary_worth_node(nodes, abstract, stopwords, extracts, stemmer)\n",
    "    edges, ist, sw = make_summary_worth_edge(edges, abstract, stopwords, extracts, stemmer)\n",
    "\n",
    "    nodes_pruned, insalientsent_pruned, sum_worthy_pruned = make_summary_worth_node(nodes_pruned, abstract, stopwords, extracts, stemmer)\n",
    "    edges_pruned, istp, swp = make_summary_worth_edge(edges_pruned, abstract, stopwords, extracts, stemmer)\n",
    "\n",
    "\n",
    "    all_data['nodes_pruned2'] = nodes_pruned\n",
    "    all_data['edges_pruned2'] = edges_pruned\n",
    "    if ground_truth:\n",
    "        all_data['nodes_sw'] = nodes\n",
    "        all_data['edges_sw'] = edges\n",
    "\n",
    "    all_data['nodes'] = nodes\n",
    "    all_data['edges'] = edges\n",
    "\n",
    "    if not test:\n",
    "        with open(os.path.join(WRITE_DIR, split, str(_id) + '.json'), 'w') as f:\n",
    "            json.dump(all_data, f)\n",
    "    else:\n",
    "        print('before:', before)\n",
    "        print('all_arguments:', all_arguments)\n",
    "        print(abstract)\n",
    "    #qgc.collect()\n",
    "    #print('{} finished'.format(_id))\n",
    "    # print('all node num:', len(all_data['nodes']))\n",
    "    # print('node num:', len(nodes))\n",
    "    return _id, len(nodes), len(nodes_pruned), len(edges), len(edges_pruned), insalientsent, sum_worthy, insalientsent_pruned, sum_worthy_pruned, \\\n",
    "           ist, sw, istp, swp\n",
    "\n",
    "\n",
    "def process_mp(split, data, stopwords, data_num, stemmer):\n",
    "    start = time.time()\n",
    "    with mp.Pool(processes=1) as pool:\n",
    "        results = list(pool.imap_unordered(process_one(split, stopwords, stemmer, ground_truth=False), list(data.items()), chunksize=1000))\n",
    "    sents = list(range(data_num))\n",
    "    processed_art = [result[0] for result in results]\n",
    "    node_num = [result[1] for result in results]\n",
    "    node_num_pruned = [result[2] for result in results]\n",
    "    edge_num = [result[3] for result in results]\n",
    "    edge_num_pruned = [result[4] for result in results]\n",
    "    ist = [result[5] for result in results]\n",
    "    sw = [result[6] for result in results]\n",
    "    istp = [result[7] for result in results]\n",
    "    swp = [result[8] for result in results]\n",
    "    iste = [result[9] for result in results]\n",
    "    swe = [result[10] for result in results]\n",
    "    istep = [result[11] for result in results]\n",
    "    swep = [result[12] for result in results]\n",
    "\n",
    "    print('average node num:', sum(node_num) / len(node_num))\n",
    "    print('average edge num:', sum(edge_num) / len(edge_num))\n",
    "    print('average node num after pruned:', sum(node_num_pruned) / len(node_num_pruned))\n",
    "    print('average edge num after pruned:', sum(edge_num_pruned) / len(edge_num_pruned))\n",
    "    print('average sum worthy nodes:', sum(sw) / len(sw))\n",
    "    print('average in salient sent nodes:', sum(ist) / len(ist))\n",
    "    print('average sum worthy nodes after pruned:', sum(swp) / len(swp))\n",
    "    print('average in salient sent nodes after pruned:', sum(istp) / len(istp))\n",
    "    print('average sum worthy edges:', sum(swe) / len(swe))\n",
    "    print('average in salient sent edges:', sum(iste) / len(iste))\n",
    "    print('average sum worthy edges after pruned:', sum(swep) / len(swep))\n",
    "    print('average in salient sent edges after pruned:', sum(istep) / len(istep))\n",
    "\n",
    "    extra_data = set(sents) - set(processed_art)\n",
    "    extra_data = list(extra_data)\n",
    "    for _id in extra_data:\n",
    "        #print('None graph id:', _id)\n",
    "        filename = os.path.join(DATA_DIR, split, str(_id) + '.json')\n",
    "        try:\n",
    "            abstract, entities, all_data, article, extracts = prepare_data(filename, split)\n",
    "            all_data['nodes'] = {}\n",
    "            all_data['edges'] = {}\n",
    "            all_data['nodes_pruned2'] = {}\n",
    "            all_data['edges_pruned2'] = {}\n",
    "            with open(os.path.join(WRITE_DIR, split, str(_id) + '.json'), 'w') as f:\n",
    "                json.dump(all_data, f)\n",
    "        except:\n",
    "            print(\"=====\",filename)\n",
    "\n",
    "\n",
    "    print('finished in {}'.format(timedelta(seconds=time.time() - start)))\n",
    "\n",
    "def process_mp_split(split, data, stopwords, data_num):\n",
    "    start = time.time()\n",
    "    data = list(data.items())\n",
    "    _split = 10\n",
    "    length = len(data)\n",
    "    for i in range(_split):\n",
    "        if i < _split - 1:\n",
    "            _input = data[:round(length / _split)]\n",
    "        else:\n",
    "            _input = data\n",
    "        with mp.Pool(processes=16) as pool:\n",
    "            result = list(pool.imap_unordered(process_one(split, stopwords, prune=False, ground_truth=True), _input, chunksize=1000))\n",
    "        if i < _split - 1:\n",
    "            data = data[round(length / _split):]\n",
    "        else:\n",
    "            del data\n",
    "        del _input\n",
    "        gc.collect()\n",
    "    sents = list(range(data_num))\n",
    "    print(sum(result) / len(result))\n",
    "\n",
    "    # extra_data = set(sents) - set(result)\n",
    "    # extra_data = list(extra_data)\n",
    "    # for _id in extra_data:\n",
    "    #     #print('None graph id:', _id)\n",
    "    #     filename = os.path.join(DATA_DIR, split, str(_id) + '.json')\n",
    "    #     abstract, entities, all_data, article = prepare_data(filename)\n",
    "    #     all_data['nodes'] = {}\n",
    "    #     all_data['edges'] = {}\n",
    "    #     with open(os.path.join(WRITE_DIR, split, str(_id) + '.json'), 'w') as f:\n",
    "    #         json.dump(all_data, f)\n",
    "\n",
    "\n",
    "    print('finished in {}'.format(timedelta(seconds=time.time() - start)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DATA_DIR = '/home/xueweiwa/Capstone/xueweiwa/data/shorten'\n",
    "OPENIE_DIR = '/home/xueweiwa/Capstone/xueweiwa/data/shorten'\n",
    "WRITE_DIR = '/home/xueweiwa/Capstone/xueweiwa/data/preprocess'\n",
    "\n",
    "\n",
    "stopwords = get_stopwods()\n",
    "stemmer = porter.PorterStemmer()\n",
    "# for split in ['test', 'val', 'train']:\n",
    "for split in ['val']:\n",
    "    if not os.path.exists(os.path.join(WRITE_DIR, split)):\n",
    "        os.makedirs(os.path.join(WRITE_DIR, split))\n",
    "    start = time.time()\n",
    "    data = read_raw(split)\n",
    "    print('finish reading data')\n",
    "    print('time elapsed:', time.time() - start)\n",
    "    print(len(data))\n",
    "    # test\n",
    "    # process_one(split, stopwords, (OPENIE_DIR + 'temp/43.txt', data[OPENIE_DIR + 'temp/43.txt']), prune=False)\n",
    "    # process_one(split, stopwords, (OPENIE_DIR + 'temp/43.txt', data[OPENIE_DIR + 'temp/43.txt']), prune=False, ground_truth=True, test=True)\n",
    "    # for i in list(data.items()):\n",
    "    #     process_one(split, stopwords, i, test=True)\n",
    "\n",
    "    # train part\n",
    "    files = glob.glob(os.path.join(DATA_DIR, split, '*'))\n",
    "    print(len(files))\n",
    "    gc.collect()\n",
    "    process_mp(split, data, stopwords, len(files), stemmer)\n",
    "    #process_mp_split(split, data, stopwords, len(files))\n",
    "\n",
    "\n",
    "\n",
    "    # post process\n",
    "    # gc.collect()\n",
    "    # data = read_raw_2(split)\n",
    "    # results = []\n",
    "    # for key, value in list(data.items()):\n",
    "    #     _id = int(key.split('/')[-1].split('.')[0])\n",
    "    #     results.append(_id)\n",
    "    # print(len(results))\n",
    "    # sents = list(range(len(files)))\n",
    "    # extra_data = set(sents) - set(results)\n",
    "    # extra_data = list(extra_data)\n",
    "    # print('extra data num:', len(extra_data))\n",
    "    # for _id in extra_data:\n",
    "    #     filename = os.path.join(DATA_DIR, split, str(_id) + '.json')\n",
    "    #     abstract, entities, all_data, article = prepare_data(filename)\n",
    "    #     all_data['nodes_pruned2'] = {}\n",
    "    #     all_data['edges_pruned2'] = {}\n",
    "    #     with open(os.path.join(WRITE_DIR, split, str(_id) + '.json'), 'w') as f:\n",
    "    #         json.dump(all_data, f)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict_items' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-2e0193d47d20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'corefs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmention\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmentions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict_items' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "\n",
    "nlp = StanfordCoreNLP(r'/home/xueweiwa/Capstone/xueweiwa/stanford-corenlp-4.1.0', quiet=False)\n",
    "props = {'annotators': 'coref', 'pipelineLanguage': 'en'}\n",
    "\n",
    "text =  \" \".join([\"the owner of an auto repair shop in michigan has become the latest business owner to claim his religious views should allow him to be able to refuse serving gay customers .\", \"brian klawiter posted a message on his company 's facebook page on tuesday in which he announced that openly gay people are not welcome at his business because he considers homosexuality to be wrong .\"])\n",
    "result = json.loads(nlp.annotate(text, properties=props))\n",
    "\n",
    "num, mentions = result['corefs'].items()[0]\n",
    "for mention in mentions:\n",
    "    print(mention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result['corefs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/xueweiwa/Capstone/xueweiwa/data/shorten/res_train862.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-406f23d354d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mOPENIE_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/xueweiwa/Capstone/xueweiwa/data/shorten'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mWRITE_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/xueweiwa/Capstone/xueweiwa/data/preprocess'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-22-406f23d354d6>\u001b[0m in \u001b[0;36mread_raw\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdata_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOPENIE_DIR\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0msplit\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'res_train862.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/xueweiwa/Capstone/xueweiwa/data/shorten/res_train862.tsv'"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "import multiprocessing as mp\n",
    "import time, glob\n",
    "from cytoolz import curry\n",
    "from datetime import timedelta\n",
    "import gc\n",
    "import os\n",
    "from nltk.stem import porter\n",
    "\n",
    "def read_raw(split):\n",
    "    data_dict = {}\n",
    "    with open(os.path.join(OPENIE_DIR,   split + 'res.tsv'), 'r') as f:\n",
    "        for line in f:\n",
    "            data = line.strip().split('\\t')\n",
    "            data = [_.strip() for _ in data]\n",
    "            try:\n",
    "                data_dict[data[0]][data[1]].append(data[2:])\n",
    "            except KeyError:\n",
    "                try:\n",
    "                    data_dict[data[0]].setdefault(data[1], [data[2:]])\n",
    "                except:\n",
    "                    data_dict[data[0]] = {data[1]: [data[2:]]}\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DATA_DIR = '/home/xueweiwa/Capstone/xueweiwa/data/shorten'\n",
    "OPENIE_DIR = '/home/xueweiwa/Capstone/xueweiwa/data/shorten'\n",
    "WRITE_DIR = '/home/xueweiwa/Capstone/xueweiwa/data/preprocess'\n",
    "data = read_raw('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "val=data['/home/xueweiwa/Capstone/xueweiwa/data/shorten/raw_val/1.json.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['0', '1', '2', '3', '4', '5', '6'])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['history',\n",
       "  'teach summer school session at',\n",
       "  'colorado college',\n",
       "  '0',\n",
       "  '1',\n",
       "  '32',\n",
       "  '39',\n",
       "  '39',\n",
       "  '41',\n",
       "  '1.000',\n",
       "  'history -- in 1893 , at the age of 33 , bates , an english professor at wellesley college , had taken a train trip to colorado springs , colorado , to teach a short summer school session at colorado college .',\n",
       "  'NN : IN CD , IN DT NN IN CD , NNS , DT NNP NN IN NNP NN , VBD VBN DT NN NN IN NN NNS , NN , TO VB DT JJ NN NN NN IN NN NN .',\n",
       "  'history',\n",
       "  'teach summer school session at',\n",
       "  'colorado college'],\n",
       " ['history',\n",
       "  'had taken',\n",
       "  'train trip',\n",
       "  '0',\n",
       "  '1',\n",
       "  '20',\n",
       "  '22',\n",
       "  '23',\n",
       "  '25',\n",
       "  '1.000',\n",
       "  'history -- in 1893 , at the age of 33 , bates , an english professor at wellesley college , had taken a train trip to colorado springs , colorado , to teach a short summer school session at colorado college .',\n",
       "  'NN : IN CD , IN DT NN IN CD , NNS , DT NNP NN IN NNP NN , VBD VBN DT NN NN IN NN NNS , NN , TO VB DT JJ NN NN NN IN NN NN .',\n",
       "  'history',\n",
       "  'have take',\n",
       "  'train trip'],\n",
       " ['history',\n",
       "  'is in',\n",
       "  '1893',\n",
       "  '0',\n",
       "  '1',\n",
       "  '2',\n",
       "  '3',\n",
       "  '3',\n",
       "  '4',\n",
       "  '1.000',\n",
       "  'history -- in 1893 , at the age of 33 , bates , an english professor at wellesley college , had taken a train trip to colorado springs , colorado , to teach a short summer school session at colorado college .',\n",
       "  'NN : IN CD , IN DT NN IN CD , NNS , DT NNP NN IN NNP NN , VBD VBN DT NN NN IN NN NNS , NN , TO VB DT JJ NN NN NN IN NN NN .',\n",
       "  'history',\n",
       "  'be in',\n",
       "  '1893'],\n",
       " ['history',\n",
       "  'teach',\n",
       "  'summer school session',\n",
       "  '0',\n",
       "  '1',\n",
       "  '32',\n",
       "  '33',\n",
       "  '35',\n",
       "  '38',\n",
       "  '1.000',\n",
       "  'history -- in 1893 , at the age of 33 , bates , an english professor at wellesley college , had taken a train trip to colorado springs , colorado , to teach a short summer school session at colorado college .',\n",
       "  'NN : IN CD , IN DT NN IN CD , NNS , DT NNP NN IN NNP NN , VBD VBN DT NN NN IN NN NNS , NN , TO VB DT JJ NN NN NN IN NN NN .',\n",
       "  'history',\n",
       "  'teach',\n",
       "  'summer school session'],\n",
       " ['history',\n",
       "  'teach',\n",
       "  'short summer school session',\n",
       "  '0',\n",
       "  '1',\n",
       "  '32',\n",
       "  '33',\n",
       "  '34',\n",
       "  '38',\n",
       "  '1.000',\n",
       "  'history -- in 1893 , at the age of 33 , bates , an english professor at wellesley college , had taken a train trip to colorado springs , colorado , to teach a short summer school session at colorado college .',\n",
       "  'NN : IN CD , IN DT NN IN CD , NNS , DT NNP NN IN NNP NN , VBD VBN DT NN NN IN NN NNS , NN , TO VB DT JJ NN NN NN IN NN NN .',\n",
       "  'history',\n",
       "  'teach',\n",
       "  'short summer school session'],\n",
       " ['age',\n",
       "  'at 1893 is',\n",
       "  'professor',\n",
       "  '7',\n",
       "  '8',\n",
       "  '5',\n",
       "  '4',\n",
       "  '15',\n",
       "  '16',\n",
       "  '1.000',\n",
       "  'history -- in 1893 , at the age of 33 , bates , an english professor at wellesley college , had taken a train trip to colorado springs , colorado , to teach a short summer school session at colorado college .',\n",
       "  'NN : IN CD , IN DT NN IN CD , NNS , DT NNP NN IN NNP NN , VBD VBN DT NN NN IN NN NNS , NN , TO VB DT JJ NN NN NN IN NN NN .',\n",
       "  'age',\n",
       "  'at 1893 be',\n",
       "  'professor'],\n",
       " ['history',\n",
       "  'had taken',\n",
       "  'train trip to colorado springs',\n",
       "  '0',\n",
       "  '1',\n",
       "  '20',\n",
       "  '22',\n",
       "  '23',\n",
       "  '28',\n",
       "  '1.000',\n",
       "  'history -- in 1893 , at the age of 33 , bates , an english professor at wellesley college , had taken a train trip to colorado springs , colorado , to teach a short summer school session at colorado college .',\n",
       "  'NN : IN CD , IN DT NN IN CD , NNS , DT NNP NN IN NNP NN , VBD VBN DT NN NN IN NN NNS , NN , TO VB DT JJ NN NN NN IN NN NN .',\n",
       "  'history',\n",
       "  'have take',\n",
       "  'train trip to colorado spring']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['they',\n",
       "  'found',\n",
       "  'they way',\n",
       "  '11',\n",
       "  '12',\n",
       "  '12',\n",
       "  '13',\n",
       "  '11',\n",
       "  '15',\n",
       "  '1.000',\n",
       "  \"several of the sights on her trip inspired her , and they found their way into her poem , including the world 's columbian exposition in chicago , the `` white city '' with its promise of the future contained within its gleaming white buildings ; the wheat fields of america 's heartland kansas , through which her train was riding on july 16 ; and the majestic view of the great plains from high atop pikes peak .\",\n",
       "  \"JJ IN DT NNS IN PRP$ NN VBD PRP , CC PRP VBD PRP$ NN IN PRP$ NN , VBG DT NN POS JJ NN IN NN , DT `` JJ NN '' IN PRP$ NN IN DT NN VBD IN PRP$ VBG JJ NNS : DT NN NNS IN NNP POS NN NNS , IN WDT PRP$ NN VBD VBG IN NNP CD : CC DT JJ NN IN DT JJ NNS IN JJ IN NNS NN .\",\n",
       "  'they',\n",
       "  'find',\n",
       "  'they way'],\n",
       " ['white city',\n",
       "  'contained within',\n",
       "  'they gleaming buildings',\n",
       "  '30',\n",
       "  '32',\n",
       "  '39',\n",
       "  '41',\n",
       "  '11',\n",
       "  '45',\n",
       "  '0.651',\n",
       "  \"several of the sights on her trip inspired her , and they found their way into her poem , including the world 's columbian exposition in chicago , the `` white city '' with its promise of the future contained within its gleaming white buildings ; the wheat fields of america 's heartland kansas , through which her train was riding on july 16 ; and the majestic view of the great plains from high atop pikes peak .\",\n",
       "  \"JJ IN DT NNS IN PRP$ NN VBD PRP , CC PRP VBD PRP$ NN IN PRP$ NN , VBG DT NN POS JJ NN IN NN , DT `` JJ NN '' IN PRP$ NN IN DT NN VBD IN PRP$ VBG JJ NNS : DT NN NNS IN NNP POS NN NNS , IN WDT PRP$ NN VBD VBG IN NNP CD : CC DT JJ NN IN DT JJ NNS IN JJ IN NNS NN .\",\n",
       "  'white city',\n",
       "  'contain within',\n",
       "  'they gleam building'],\n",
       " ['white city',\n",
       "  'is with',\n",
       "  'its promise of future',\n",
       "  '30',\n",
       "  '32',\n",
       "  '33',\n",
       "  '34',\n",
       "  '34',\n",
       "  '39',\n",
       "  '1.000',\n",
       "  \"several of the sights on her trip inspired her , and they found their way into her poem , including the world 's columbian exposition in chicago , the `` white city '' with its promise of the future contained within its gleaming white buildings ; the wheat fields of america 's heartland kansas , through which her train was riding on july 16 ; and the majestic view of the great plains from high atop pikes peak .\",\n",
       "  \"JJ IN DT NNS IN PRP$ NN VBD PRP , CC PRP VBD PRP$ NN IN PRP$ NN , VBG DT NN POS JJ NN IN NN , DT `` JJ NN '' IN PRP$ NN IN DT NN VBD IN PRP$ VBG JJ NNS : DT NN NNS IN NNP POS NN NNS , IN WDT PRP$ NN VBD VBG IN NNP CD : CC DT JJ NN IN DT JJ NNS IN JJ IN NNS NN .\",\n",
       "  'white city',\n",
       "  'be with',\n",
       "  'its promise of future'],\n",
       " ['city',\n",
       "  'contained within',\n",
       "  'they gleaming white buildings',\n",
       "  '31',\n",
       "  '32',\n",
       "  '39',\n",
       "  '41',\n",
       "  '11',\n",
       "  '45',\n",
       "  '0.651',\n",
       "  \"several of the sights on her trip inspired her , and they found their way into her poem , including the world 's columbian exposition in chicago , the `` white city '' with its promise of the future contained within its gleaming white buildings ; the wheat fields of america 's heartland kansas , through which her train was riding on july 16 ; and the majestic view of the great plains from high atop pikes peak .\",\n",
       "  \"JJ IN DT NNS IN PRP$ NN VBD PRP , CC PRP VBD PRP$ NN IN PRP$ NN , VBG DT NN POS JJ NN IN NN , DT `` JJ NN '' IN PRP$ NN IN DT NN VBD IN PRP$ VBG JJ NNS : DT NN NNS IN NNP POS NN NNS , IN WDT PRP$ NN VBD VBG IN NNP CD : CC DT JJ NN IN DT JJ NNS IN JJ IN NNS NN .\",\n",
       "  'city',\n",
       "  'contain within',\n",
       "  'they gleam white building'],\n",
       " ['america',\n",
       "  'of',\n",
       "  'kansas',\n",
       "  '50',\n",
       "  '51',\n",
       "  '49',\n",
       "  '50',\n",
       "  '53',\n",
       "  '54',\n",
       "  '1.000',\n",
       "  \"several of the sights on her trip inspired her , and they found their way into her poem , including the world 's columbian exposition in chicago , the `` white city '' with its promise of the future contained within its gleaming white buildings ; the wheat fields of america 's heartland kansas , through which her train was riding on july 16 ; and the majestic view of the great plains from high atop pikes peak .\",\n",
       "  \"JJ IN DT NNS IN PRP$ NN VBD PRP , CC PRP VBD PRP$ NN IN PRP$ NN , VBG DT NN POS JJ NN IN NN , DT `` JJ NN '' IN PRP$ NN IN DT NN VBD IN PRP$ VBG JJ NNS : DT NN NNS IN NNP POS NN NNS , IN WDT PRP$ NN VBD VBG IN NNP CD : CC DT JJ NN IN DT JJ NNS IN JJ IN NNS NN .\",\n",
       "  'america',\n",
       "  'of',\n",
       "  'kansa'],\n",
       " ['sights',\n",
       "  'inspired',\n",
       "  'her',\n",
       "  '3',\n",
       "  '4',\n",
       "  '7',\n",
       "  '8',\n",
       "  '5',\n",
       "  '6',\n",
       "  '1.000',\n",
       "  \"several of the sights on her trip inspired her , and they found their way into her poem , including the world 's columbian exposition in chicago , the `` white city '' with its promise of the future contained within its gleaming white buildings ; the wheat fields of america 's heartland kansas , through which her train was riding on july 16 ; and the majestic view of the great plains from high atop pikes peak .\",\n",
       "  \"JJ IN DT NNS IN PRP$ NN VBD PRP , CC PRP VBD PRP$ NN IN PRP$ NN , VBG DT NN POS JJ NN IN NN , DT `` JJ NN '' IN PRP$ NN IN DT NN VBD IN PRP$ VBG JJ NNS : DT NN NNS IN NNP POS NN NNS , IN WDT PRP$ NN VBD VBG IN NNP CD : CC DT JJ NN IN DT JJ NNS IN JJ IN NNS NN .\",\n",
       "  'sight',\n",
       "  'inspire',\n",
       "  'she'],\n",
       " ['city',\n",
       "  'contained within',\n",
       "  'they buildings',\n",
       "  '31',\n",
       "  '32',\n",
       "  '39',\n",
       "  '41',\n",
       "  '11',\n",
       "  '45',\n",
       "  '0.651',\n",
       "  \"several of the sights on her trip inspired her , and they found their way into her poem , including the world 's columbian exposition in chicago , the `` white city '' with its promise of the future contained within its gleaming white buildings ; the wheat fields of america 's heartland kansas , through which her train was riding on july 16 ; and the majestic view of the great plains from high atop pikes peak .\",\n",
       "  \"JJ IN DT NNS IN PRP$ NN VBD PRP , CC PRP VBD PRP$ NN IN PRP$ NN , VBG DT NN POS JJ NN IN NN , DT `` JJ NN '' IN PRP$ NN IN DT NN VBD IN PRP$ VBG JJ NNS : DT NN NNS IN NNP POS NN NNS , IN WDT PRP$ NN VBD VBG IN NNP CD : CC DT JJ NN IN DT JJ NNS IN JJ IN NNS NN .\",\n",
       "  'city',\n",
       "  'contain within',\n",
       "  'they building'],\n",
       " ['her train',\n",
       "  'was riding on',\n",
       "  'july 16',\n",
       "  '5',\n",
       "  '59',\n",
       "  '59',\n",
       "  '62',\n",
       "  '62',\n",
       "  '64',\n",
       "  '1.000',\n",
       "  \"several of the sights on her trip inspired her , and they found their way into her poem , including the world 's columbian exposition in chicago , the `` white city '' with its promise of the future contained within its gleaming white buildings ; the wheat fields of america 's heartland kansas , through which her train was riding on july 16 ; and the majestic view of the great plains from high atop pikes peak .\",\n",
       "  \"JJ IN DT NNS IN PRP$ NN VBD PRP , CC PRP VBD PRP$ NN IN PRP$ NN , VBG DT NN POS JJ NN IN NN , DT `` JJ NN '' IN PRP$ NN IN DT NN VBD IN PRP$ VBG JJ NNS : DT NN NNS IN NNP POS NN NNS , IN WDT PRP$ NN VBD VBG IN NNP CD : CC DT JJ NN IN DT JJ NNS IN JJ IN NNS NN .\",\n",
       "  'she train',\n",
       "  'be ride on',\n",
       "  'july 16'],\n",
       " ['world',\n",
       "  'including',\n",
       "  'columbian exposition in chicago',\n",
       "  '21',\n",
       "  '22',\n",
       "  '19',\n",
       "  '20',\n",
       "  '23',\n",
       "  '27',\n",
       "  '1.000',\n",
       "  \"several of the sights on her trip inspired her , and they found their way into her poem , including the world 's columbian exposition in chicago , the `` white city '' with its promise of the future contained within its gleaming white buildings ; the wheat fields of america 's heartland kansas , through which her train was riding on july 16 ; and the majestic view of the great plains from high atop pikes peak .\",\n",
       "  \"JJ IN DT NNS IN PRP$ NN VBD PRP , CC PRP VBD PRP$ NN IN PRP$ NN , VBG DT NN POS JJ NN IN NN , DT `` JJ NN '' IN PRP$ NN IN DT NN VBD IN PRP$ VBG JJ NNS : DT NN NNS IN NNP POS NN NNS , IN WDT PRP$ NN VBD VBG IN NNP CD : CC DT JJ NN IN DT JJ NNS IN JJ IN NNS NN .\",\n",
       "  'world',\n",
       "  'include',\n",
       "  'columbian exposition in chicago'],\n",
       " ['white city',\n",
       "  'contained within',\n",
       "  'they gleaming white buildings',\n",
       "  '30',\n",
       "  '32',\n",
       "  '39',\n",
       "  '41',\n",
       "  '11',\n",
       "  '45',\n",
       "  '0.651',\n",
       "  \"several of the sights on her trip inspired her , and they found their way into her poem , including the world 's columbian exposition in chicago , the `` white city '' with its promise of the future contained within its gleaming white buildings ; the wheat fields of america 's heartland kansas , through which her train was riding on july 16 ; and the majestic view of the great plains from high atop pikes peak .\",\n",
       "  \"JJ IN DT NNS IN PRP$ NN VBD PRP , CC PRP VBD PRP$ NN IN PRP$ NN , VBG DT NN POS JJ NN IN NN , DT `` JJ NN '' IN PRP$ NN IN DT NN VBD IN PRP$ VBG JJ NNS : DT NN NNS IN NNP POS NN NNS , IN WDT PRP$ NN VBD VBG IN NNP CD : CC DT JJ NN IN DT JJ NNS IN JJ IN NNS NN .\",\n",
       "  'white city',\n",
       "  'contain within',\n",
       "  'they gleam white building'],\n",
       " ['white city',\n",
       "  'contained within',\n",
       "  'they buildings',\n",
       "  '30',\n",
       "  '32',\n",
       "  '39',\n",
       "  '41',\n",
       "  '11',\n",
       "  '45',\n",
       "  '0.651',\n",
       "  \"several of the sights on her trip inspired her , and they found their way into her poem , including the world 's columbian exposition in chicago , the `` white city '' with its promise of the future contained within its gleaming white buildings ; the wheat fields of america 's heartland kansas , through which her train was riding on july 16 ; and the majestic view of the great plains from high atop pikes peak .\",\n",
       "  \"JJ IN DT NNS IN PRP$ NN VBD PRP , CC PRP VBD PRP$ NN IN PRP$ NN , VBG DT NN POS JJ NN IN NN , DT `` JJ NN '' IN PRP$ NN IN DT NN VBD IN PRP$ VBG JJ NNS : DT NN NNS IN NNP POS NN NNS , IN WDT PRP$ NN VBD VBG IN NNP CD : CC DT JJ NN IN DT JJ NNS IN JJ IN NNS NN .\",\n",
       "  'white city',\n",
       "  'contain within',\n",
       "  'they building'],\n",
       " [\"world 's columbian exposition\",\n",
       "  'is in',\n",
       "  'chicago',\n",
       "  '21',\n",
       "  '25',\n",
       "  '25',\n",
       "  '26',\n",
       "  '26',\n",
       "  '27',\n",
       "  '1.000',\n",
       "  \"several of the sights on her trip inspired her , and they found their way into her poem , including the world 's columbian exposition in chicago , the `` white city '' with its promise of the future contained within its gleaming white buildings ; the wheat fields of america 's heartland kansas , through which her train was riding on july 16 ; and the majestic view of the great plains from high atop pikes peak .\",\n",
       "  \"JJ IN DT NNS IN PRP$ NN VBD PRP , CC PRP VBD PRP$ NN IN PRP$ NN , VBG DT NN POS JJ NN IN NN , DT `` JJ NN '' IN PRP$ NN IN DT NN VBD IN PRP$ VBG JJ NNS : DT NN NNS IN NNP POS NN NNS , IN WDT PRP$ NN VBD VBG IN NNP CD : CC DT JJ NN IN DT JJ NNS IN JJ IN NNS NN .\",\n",
       "  \"world 's columbian exposition\",\n",
       "  'be in',\n",
       "  'chicago'],\n",
       " ['white city',\n",
       "  'contained within',\n",
       "  'they white buildings',\n",
       "  '30',\n",
       "  '32',\n",
       "  '39',\n",
       "  '41',\n",
       "  '11',\n",
       "  '45',\n",
       "  '0.651',\n",
       "  \"several of the sights on her trip inspired her , and they found their way into her poem , including the world 's columbian exposition in chicago , the `` white city '' with its promise of the future contained within its gleaming white buildings ; the wheat fields of america 's heartland kansas , through which her train was riding on july 16 ; and the majestic view of the great plains from high atop pikes peak .\",\n",
       "  \"JJ IN DT NNS IN PRP$ NN VBD PRP , CC PRP VBD PRP$ NN IN PRP$ NN , VBG DT NN POS JJ NN IN NN , DT `` JJ NN '' IN PRP$ NN IN DT NN VBD IN PRP$ VBG JJ NNS : DT NN NNS IN NNP POS NN NNS , IN WDT PRP$ NN VBD VBG IN NNP CD : CC DT JJ NN IN DT JJ NNS IN JJ IN NNS NN .\",\n",
       "  'white city',\n",
       "  'contain within',\n",
       "  'they white building'],\n",
       " ['city',\n",
       "  'contained within',\n",
       "  'they gleaming buildings',\n",
       "  '31',\n",
       "  '32',\n",
       "  '39',\n",
       "  '41',\n",
       "  '11',\n",
       "  '45',\n",
       "  '0.651',\n",
       "  \"several of the sights on her trip inspired her , and they found their way into her poem , including the world 's columbian exposition in chicago , the `` white city '' with its promise of the future contained within its gleaming white buildings ; the wheat fields of america 's heartland kansas , through which her train was riding on july 16 ; and the majestic view of the great plains from high atop pikes peak .\",\n",
       "  \"JJ IN DT NNS IN PRP$ NN VBD PRP , CC PRP VBD PRP$ NN IN PRP$ NN , VBG DT NN POS JJ NN IN NN , DT `` JJ NN '' IN PRP$ NN IN DT NN VBD IN PRP$ VBG JJ NNS : DT NN NNS IN NNP POS NN NNS , IN WDT PRP$ NN VBD VBG IN NNP CD : CC DT JJ NN IN DT JJ NNS IN JJ IN NNS NN .\",\n",
       "  'city',\n",
       "  'contain within',\n",
       "  'they gleam building'],\n",
       " ['city',\n",
       "  'contained within',\n",
       "  'they white buildings',\n",
       "  '31',\n",
       "  '32',\n",
       "  '39',\n",
       "  '41',\n",
       "  '11',\n",
       "  '45',\n",
       "  '0.651',\n",
       "  \"several of the sights on her trip inspired her , and they found their way into her poem , including the world 's columbian exposition in chicago , the `` white city '' with its promise of the future contained within its gleaming white buildings ; the wheat fields of america 's heartland kansas , through which her train was riding on july 16 ; and the majestic view of the great plains from high atop pikes peak .\",\n",
       "  \"JJ IN DT NNS IN PRP$ NN VBD PRP , CC PRP VBD PRP$ NN IN PRP$ NN , VBG DT NN POS JJ NN IN NN , DT `` JJ NN '' IN PRP$ NN IN DT NN VBD IN PRP$ VBG JJ NNS : DT NN NNS IN NNP POS NN NNS , IN WDT PRP$ NN VBD VBG IN NNP CD : CC DT JJ NN IN DT JJ NNS IN JJ IN NNS NN .\",\n",
       "  'city',\n",
       "  'contain within',\n",
       "  'they white building']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
